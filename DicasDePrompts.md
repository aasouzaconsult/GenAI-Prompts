# Dicas de Prompts üíª>

![](https://blogdozouza.wordpress.com/wp-content/uploads/2024/11/contexto.png)

## Persona
Pense em quem voc√™ quer que a IA seja ao responder. Pode ser um especialista, um recrutador, um escritor, um profissional de dados ou at√© mesmo um personagem fict√≠cio. 

**Por exemplo**: Voc√™ √© um especialista em an√°lise de dados e trabalha no setor financeiro de bigtechs...

## Clear Instructions
Gere instru√ß√µes claras e seja espec√≠fico, forne√ßa mais detalhes (ela (IA) n√£o sabe o que voc√™ t√° precisando). Seja claro sobre qual √© o seu objetivo final. 

*Comece sua pergunta com um verbo de a√ß√£o, como ‚Äúgerar‚Äù, ‚Äúdar‚Äù, ‚Äúescrever‚Äù ou ‚Äúanalisar‚Äù.

**Por exemplo:** Escreva um resumo do relat√≥rio de vendas trimestrais. Um exemplo se encontra abaixo...

## Formato
Visualize como voc√™ deseja que a resposta seja formatada. Pode ser uma lista, um e-mail, um resumo ou qualquer formato que voc√™ precise. Ou ainda, o tamanho da mensagem de retorno...

**Por exemplo**: Forne√ßa as informa√ß√µes em formato de lista com marcadores.

## Contexto (Limit the scope)
O contexto √© fundamental para restringir as possibilidades. Forne√ßa informa√ß√µes sobre o hist√≥rico do usu√°rio, o que define o sucesso e o ambiente em que est√£o. 

**Por exemplo:** Voc√™ √© um analista de dados e tem um cliente que deseja minimizar custos.

## Exemplos (*Few-Shot Prompts*)
Incluir exemplos na sua pergunta √© uma pr√°tica recomendada. Exemplos tornam mais f√°cil para a IA entender o que voc√™ deseja.

**Por exemplo:** D√™ um KPI relacionado ao setor financeiro referente a margem bruta por semana e que seja r√°pido de implementar...

## Tom
O tom da resposta √© importante. Voc√™ pode pedir um tom formal, casual, entusi√°stico ou at√© pessimista. 

**Por exemplo:** Forne√ßa as informa√ß√µes em formato de lista com marcadores, escreva com um tom motivador e entusiasmado!

[mais dicas](https://github.com/aasouzaconsult/GenAI-Prompts/blob/main/DicasDePrompts_mais.md)

# T√©cnicas de Prompts üíª>
[Mais informa√ß√µes e detalhes](https://www.promptingguide.ai/techniques)

## Zero-Shot Prompts
Quando n√£o √© necess√°rio nenhum exemplo, o pr√≥prio modelo j√° pode lhe dar respostas corretas.

**Por exemplo:** 
```
When is Christmas in America?
```

## Few-Shot Prompts
Quando voc√™ d√° um contexto, d√° exemplos para que a resposta saia ou seja mais precisa. [more informations](https://www.promptingguide.ai/techniques/fewshot) | [artigo](https://arxiv.org/pdf/2005.14165)

**Por exemplo:** 
```
What is Alex¬¥s favourite type of foods?
```
*N√£o saber√° responder, pois ele n√£o conhece meus gostos!*

```
Alex¬¥s favourite type of foods include: Burges, Pizza, Fries, Chocolate
What restaurant should I take Alex to in Dubai this weekend?
```
*O prompt acima, provavelmente ir√° lhe retornar um resultado mais assertivo.*

## Chain-of-Thought (CoT) Prompting
Introduzido em Wei et al. (2022), a solicita√ß√£o de cadeia de pensamento ([CoT](https://www.promptingguide.ai/techniques/cot)) permite recursos de racioc√≠nio complexos por meio de etapas intermedi√°rias de racioc√≠nio. Voc√™ pode combin√°-lo com solicita√ß√µes r√°pidas para obter melhores resultados em tarefas mais complexas que exigem racioc√≠nio antes de responder. [artigo](https://arxiv.org/abs/2201.11903)

```
Os n√∫meros √≠mpares neste grupo somam um n√∫mero par: 4, 8, 9, 15, 12, 2, 1.
R: Somando todos os n√∫meros √≠mpares (9, 15, 1) d√° 25. A resposta √© falsa.

Os n√∫meros √≠mpares neste grupo somam um n√∫mero par: 17, 10, 19, 4, 8, 12, 24.
R: Somando todos os n√∫meros √≠mpares (17, 19) d√° 36. A resposta √© Verdadeira.

Os n√∫meros √≠mpares neste grupo somam um n√∫mero par: 16, 11, 14, 4, 8, 13, 24.
R: Somando todos os n√∫meros √≠mpares (11, 13) d√° 24. A resposta √© Verdadeira.

Os n√∫meros √≠mpares neste grupo somam um n√∫mero par: 17, 9, 10, 12, 13, 4, 2.
R: Somando todos os n√∫meros √≠mpares (17, 9, 13) d√° 39. A resposta √© falsa.

Os n√∫meros √≠mpares neste grupo somam um n√∫mero par: 15, 32, 5, 13, 82, 7, 1.
A:
```
Result: Adding all the odd numbers (15, 5, 13, 7, 1) gives 41. The answer is False.


## Self-Consistency
Talvez uma das t√©cnicas mais avan√ßadas dispon√≠veis para *engenharia de prompt* seja a autoconsist√™ncia. Proposto por [Wang et al. (2022) - artigo](https://arxiv.org/abs/2203.11171), a autoconsist√™ncia visa "substituir a decodifica√ß√£o ing√™nua e gananciosa usada na estimula√ß√£o da cadeia de pensamento". A ideia √© provar m√∫ltiplos e diversos caminhos de racioc√≠nio por meio de CoT de poucas tentativas e usar as gera√ß√µes para selecionar a resposta mais consistente. Isso ajuda a aumentar o desempenho das solicita√ß√µes do CoT em tarefas que envolvem racioc√≠nio aritm√©tico e de bom senso.

```
Quando eu tinha 6 anos, minha irm√£ tinha metade da minha idade. Agora
Tenho 70 anos, quantos anos tem minha irm√£?
```
Talvez d√™ uma resposta errada, no meu teste deu certo kkk - GTP 3.5 Turbo

Aqui dando alguns exemplos, a ideia √© rodar algumas vezes, e pegar a resposta que mais apareceu (o meu todas as respostas foram iguais)
```
P: Existem 15 √°rvores no bosque. Os trabalhadores do bosque plantar√£o √°rvores no bosque hoje. Depois de terminarem,
haver√° 21 √°rvores. Quantas √°rvores os trabalhadores do bosque plantaram hoje?
R: Come√ßamos com 15 √°rvores. Mais tarde temos 21 √°rvores. A diferen√ßa deve ser o n√∫mero de √°rvores plantadas.
Ent√£o, eles devem ter plantado 21 ‚Äì 15 = 6 √°rvores. A resposta √© 6.
P: Se houver 3 carros no estacionamento e chegarem mais 2 carros, quantos carros est√£o no estacionamento?
R: J√° existem 3 carros no estacionamento. Chegam mais 2. Agora existem 3 + 2 = 5 carros. A resposta √© 5.
P: Leah comeu 32 chocolates e sua irm√£ 42. Se elas comeram 35, quantos peda√ßos sobraram no total?
R: Leah tinha 32 chocolates e a irm√£ de Leah tinha 42. Isso significa que originalmente havia 32 + 42 = 74
chocolates. 35 foram comidos. Ent√£o, no total, eles ainda t√™m 74 ‚Äì 35 = 39 chocolates. A resposta √© 39.
P: Jason comeu 20 pirulitos. Ele deu alguns pirulitos para Denny. Agora Jason tem 12 pirulitos. Quantos pirulitos
Jason deu para Denny?
R: Jason comeu 20 pirulitos. Como ele s√≥ tem 12 agora, deve ter dado o resto para Denny. O n√∫mero de
os pirulitos que ele deu ao Denny devem ter sido 20 - 12 = 8 pirulitos. A resposta √© 8.
P: Shawn tem cinco brinquedos. No Natal, ele ganhou dois brinquedos da m√£e e do pai. Quantos brinquedos tem
ele tem agora?
R: Ele tem 5 brinquedos. Ele ganhou 2 da m√£e, ent√£o depois disso ele tem 5 + 2 = 7 brinquedos. Ent√£o ele ganhou mais 2 do pai, ent√£o
no total ele tem 7 + 2 = 9 brinquedos. A resposta √© 9.
P: Havia nove computadores na sala do servidor. Mais cinco computadores foram instalados por dia, desde
segunda a quinta. Quantos computadores est√£o agora na sala do servidor?
R: S√£o 4 dias de segunda a quinta. 5 computadores foram adicionados a cada dia. Isso significa no total 4 * 5 =
20 computadores foram adicionados. No in√≠cio havia 9 computadores, ent√£o agora existem 9 + 20 = 29 computadores.
A resposta √© 29.
P: Michael tinha 58 bolas de golfe. Na ter√ßa-feira, ele perdeu 23 bolas de golfe. Na quarta-feira, ele perdeu mais 2. Quantos
bolas de golfe ele tinha no final da quarta-feira?
R: Michael inicialmente tinha 58 bolas. Ele perdeu 23 na ter√ßa-feira, ent√£o depois disso ele tem 58 - 23 = 35 bolas. Sobre
Quarta-feira ele perdeu mais 2, ent√£o agora tem 35 - 2 = 33 bolas. A resposta √© 33.
P: Olivia tem $ 23. Ela comprou cinco bagels por US$ 3 cada. Quanto dinheiro ela ainda tem?
R: Ela comprou 5 bagels por US$ 3 cada. Isso significa que ela gastou US$ 15. Ela ainda tem $ 8.
P: Quando eu tinha 6 anos, minha irm√£ tinha metade da minha idade. Agora tenho 70 anos, quantos anos minha irm√£ tem?
R:
```

## Generate Knowledge Prompting
No veu ver, √© dar contexto - [More informattion](https://www.promptingguide.ai/techniques/knowledge)

## Prompt Chaining (Encadeamento de prompt)
Usar o resultado de um prompt para entrada em outro

## Tree of Thoughts (ToT)
Para tarefas complexas que exigem explora√ß√£o ou vis√£o estrat√©gica, as t√©cnicas de solicita√ß√£o tradicionais ou simples s√£o insuficientes. Yao e outros. (2023) e Longo (2023)prop√¥s recentemente a √Årvore de Pensamentos (ToT), uma estrutura que generaliza a sugest√£o de cadeia de pensamentos e incentiva a explora√ß√£o de pensamentos que servem como etapas intermedi√°rias para a resolu√ß√£o geral de problemas com modelos de linguagem.

ToT mant√©m uma √°rvore de pensamentos, onde os pensamentos representam sequ√™ncias de linguagem coerentes que servem como etapas intermedi√°rias para a resolu√ß√£o de um problema. Esta abordagem permite que um LM autoavalie o progresso que os pensamentos intermedi√°rios fazem para resolver um problema atrav√©s de um processo de racioc√≠nio deliberado. A capacidade do LM de gerar e avaliar pensamentos √© ent√£o combinada com algoritmos de busca (por exemplo, busca em largura e busca em profundidade) para permitir a explora√ß√£o sistem√°tica de pensamentos com antecipa√ß√£o e retrocesso. [artigo](https://arxiv.org/abs/2305.10601)

```
Imagine que tr√™s especialistas diferentes estejam respondendo a essa pergunta.
Todos os especialistas anotar√£o uma etapa de seu pensamento,
depois compartilhe com o grupo.
Em seguida, todos os especialistas passar√£o para a pr√≥xima etapa, etc.
Se algum especialista perceber que est√° errado em algum momento, ele vai embora.
A quest√£o √©...

Bob est√° na sala.
Ele caminha at√© a cozinha carregando uma x√≠cara.
Ele coloca uma bola na x√≠cara e leva a x√≠cara para o quarto.
Ele vira a x√≠cara de cabe√ßa para baixo e vai at√© o jardim.
Ele coloca a x√≠cara no jardim e vai at√© a garagem.
Onde est√° a bola?
```
*Answer:* A bola esta no Quarto

ou 
```
"Tr√™s especialistas est√£o discutindo onde a bola foi deixada. Cada especialista ir√° registrar um racioc√≠nio por vez e, em seguida, compartilhar√° com os outros. Ap√≥s cada rodada, eles podem ajustar suas respostas com base no que observaram. Se algum especialista perceber que est√° em um caminho errado, ele abandonar√° a discuss√£o.

A situa√ß√£o √© a seguinte:

Bob est√° na sala.
Ele caminha at√© a cozinha carregando uma x√≠cara.
Ele coloca uma bola na x√≠cara e leva a xicara para o quarto.
Ele vira a x√≠cara de cabe√ßa para baixo e vai at√© o jardim.
Ele coloca a x√≠cara no jardim e vai at√© a garagem.
Onde est√° a bola? "

Narre o pensamento dos especialistas!
```
* [Gemini](https://g.co/gemini/share/334eb85f6e7d) | [ChatGPT](https://chatgpt.com/share/67293253-eb04-800a-b6e7-f80ddd830158)

## Directional Stimulus Prompting
- [artigo](https://arxiv.org/abs/2302.11520)
- [more](https://www.promptingguide.ai/techniques/dsp)

## Skeleton of Thought
- [artigo](https://arxiv.org/abs/2307.15337)
  
## Generated Knowledge Prompting
- [artigo](https://arxiv.org/abs/2110.08387)
  
## Maieutic Prompting
- [artigo](https://arxiv.org/abs/2205.11822)

## Automatic Reasoning and Tool-use (ART) - Racioc√≠nio Autom√°tico e Uso de Ferramentas
Combinar sugest√µes e ferramentas CoT de maneira intercalada mostrou ser uma abordagem forte e robusta para abordar muitas tarefas com LLMs. Essas abordagens normalmente exigem demonstra√ß√µes espec√≠ficas de tarefas elaboradas manualmente e intercala√ß√£o cuidadosamente planejada de gera√ß√µes de modelos com o uso de ferramentas. Paranjape et al., (2023)propor uma nova estrutura que usa um LLM congelado para gerar automaticamente etapas intermedi√°rias de racioc√≠nio como um programa.

## Automatic Prompt Engineer (APE)
[Este artigo](https://www.promptingguide.ai/techniques/ape) aborda um t√≥pico importante relacionado √† engenharia de prompts, que √© a ideia de otimizar prompts automaticamente. Embora n√£o nos aprofundemos neste t√≥pico neste guia, aqui est√£o alguns artigos importantes se voc√™ estiver interessado no t√≥pico:
- [Prompt-OIRL](https://arxiv.org/abs/2309.06553)- prop√µe o uso de aprendizagem por refor√ßo inverso offline para gerar prompts dependentes de consulta.
- [OPRO](https://arxiv.org/abs/2309.03409)- apresenta a ideia de usar LLMs para otimizar prompts: deixe os LLMs "Respirar fundo" melhora o desempenho em problemas matem√°ticos.
- [AutoPrompt](https://arxiv.org/abs/2010.15980)- prop√µe uma abordagem para criar automaticamente prompts para um conjunto diversificado de tarefas com base na pesquisa guiada por gradiente.
- [Prefix Tuning](https://arxiv.org/abs/2101.00190)- uma alternativa leve ao ajuste fino que precede um prefixo cont√≠nuo trein√°vel para tarefas NLG.
- [Prompt Tuning](https://arxiv.org/abs/2104.08691)- prop√µe um mecanismo para aprender soft prompts por meio de retropropaga√ß√£o.

## Active Prompt
Os m√©todos de cadeia de pensamento (CoT) dependem de um conjunto fixo de exemplares anotados por humanos. O problema com isto √© que os exemplares podem n√£o ser os exemplos mais eficazes para as diferentes tarefas. Para resolver isso, Diao et al., (2023)prop√¥s recentemente uma nova abordagem de prompt chamada Active-Prompt para adaptar LLMs a diferentes exemplos de prompts espec√≠ficos de tarefas (anotados com racioc√≠nio CoT projetado por humanos).

![Veja a Ilustra√ß√£o](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Factive-prompt.f739657b.png&w=1200&q=75)
O primeiro passo √© consultar o LLM com ou sem alguns exemplos de CoT. k respostas poss√≠veis s√£o geradas para um conjunto de perguntas de treinamento. Uma m√©trica de incerteza √© calculada com base nas k respostas (discord√¢ncia utilizada). As quest√µes mais incertas s√£o selecionadas para anota√ß√£o por humanos. Os novos exemplares anotados s√£o ent√£o usados ‚Äã‚Äãpara inferir cada quest√£o.

## Program-aided Language Model (PAL)
- [artigo](https://arxiv.org/abs/2211.10435)
- [more](https://www.promptingguide.ai/techniques/pal)

### Example 
*N√£o testei a fundo (deu erro no primeiro trecho)*

```python
import openai
from datetime import datetime
from dateutil.relativedelta import relativedelta
import os
from langchain.llms import OpenAI
from dotenv import load_dotenv
```

```python
load_dotenv()
 
# API configuration
openai.api_key = os.getenv("OPENAI_API_KEY")
 
# for LangChain
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
```

```python
llm = OpenAI(model_name='text-davinci-003', temperature=0)
```

```python
question = "Today is 27 February 2023. I was born exactly 25 years ago. What is the date I was born in MM/DD/YYYY?"
 
DATE_UNDERSTANDING_PROMPT = """
# Q: 2015 is coming in 36 hours. What is the date one week from today in MM/DD/YYYY?
# If 2015 is coming in 36 hours, then today is 36 hours before.
today = datetime(2015, 1, 1) - relativedelta(hours=36)
# One week from today,
one_week_from_today = today + relativedelta(weeks=1)
# The answer formatted with %m/%d/%Y is
one_week_from_today.strftime('%m/%d/%Y')
# Q: The first day of 2019 is a Tuesday, and today is the first Monday of 2019. What is the date today in MM/DD/YYYY?
# If the first day of 2019 is a Tuesday, and today is the first Monday of 2019, then today is 6 days later.
today = datetime(2019, 1, 1) + relativedelta(days=6)
# The answer formatted with %m/%d/%Y is
today.strftime('%m/%d/%Y')
# Q: The concert was scheduled to be on 06/01/1943, but was delayed by one day to today. What is the date 10 days ago in MM/DD/YYYY?
# If the concert was scheduled to be on 06/01/1943, but was delayed by one day to today, then today is one day later.
today = datetime(1943, 6, 1) + relativedelta(days=1)
# 10 days ago,
ten_days_ago = today - relativedelta(days=10)
# The answer formatted with %m/%d/%Y is
ten_days_ago.strftime('%m/%d/%Y')
# Q: It is 4/19/1969 today. What is the date 24 hours later in MM/DD/YYYY?
# It is 4/19/1969 today.
today = datetime(1969, 4, 19)
# 24 hours later,
later = today + relativedelta(hours=24)
# The answer formatted with %m/%d/%Y is
today.strftime('%m/%d/%Y')
# Q: Jane thought today is 3/11/2002, but today is in fact Mar 12, which is 1 day later. What is the date 24 hours later in MM/DD/YYYY?
# If Jane thought today is 3/11/2002, but today is in fact Mar 12, then today is 3/12/2002.
today = datetime(2002, 3, 12)
# 24 hours later,
later = today + relativedelta(hours=24)
# The answer formatted with %m/%d/%Y is
later.strftime('%m/%d/%Y')
# Q: Jane was born on the last day of Feburary in 2001. Today is her 16-year-old birthday. What is the date yesterday in MM/DD/YYYY?
# If Jane was born on the last day of Feburary in 2001 and today is her 16-year-old birthday, then today is 16 years later.
today = datetime(2001, 2, 28) + relativedelta(years=16)
# Yesterday,
yesterday = today - relativedelta(days=1)
# The answer formatted with %m/%d/%Y is
yesterday.strftime('%m/%d/%Y')
# Q: {question}
""".strip() + '\n'
```

```python
llm_out = llm(DATE_UNDERSTANDING_PROMPT.format(question=question))
print(llm_out)
```

```python
exec(llm_out)
print(born)
```

## ReAct
- [artigo](https://arxiv.org/abs/2210.03629)
- [more](https://www.promptingguide.ai/techniques/react)

![](https://blogdozouza.files.wordpress.com/2024/01/screenshot_14.png)

![](https://blogdozouza.files.wordpress.com/2024/01/screenshot_15.png)

## Multimodal CoT Prompting
- [more](https://www.promptingguide.ai/techniques/multimodalcot)
- [article](https://arxiv.org/abs/2302.00923)

## GraphPrompt
- [article](https://arxiv.org/abs/2302.08043)

# Ferramentas
- Dyno (Prompt Enginnering IDE)
- Dust
- LangChain
- Prompttable

---

## RAG - Retrieval Augment Generation (Gera√ß√£o Aumentada de Recupera√ß√£o)
RAG combina um componente de recupera√ß√£o de informa√ß√£o com um modelo gerador de texto. O RAG pode ser ajustado e seu conhecimento interno modificado de forma eficiente e sem a necessidade de retreinamento de todo o modelo. [more informations](https://medium.com/blog-do-zouza/rag-retrieval-augmented-generation-8238a20e381d)

A t√©cnica de Retrieval-Augmented Generation (RAG) aprimora os resultados de um LLM ao integrar informa√ß√µes espec√≠ficas e atualizadas, sem a necessidade de alterar o modelo de IA subjacente. 
Enriquece seus prompts com informa√ß√µes/documentos da sua empresa...
![](https://blogdozouza.wordpress.com/wp-content/uploads/2024/11/rag.png)

## Fine tunning
√â como dar um "treinamento especial" para que ele se torne especialista em uma tarefa espec√≠fica. Isso permite que o modelo aprenda a realizar tarefas complexas com mais precis√£o e criatividade, como escrever textos no estilo de um autor espec√≠fico ou gerar c√≥digo de programa√ß√£o para diferentes aplica√ß√µes.

![](https://blogdozouza.wordpress.com/wp-content/uploads/2024/11/finetunning1.png)

![](https://blogdozouza.wordpress.com/wp-content/uploads/2024/11/finetunning2-1.png)

---
   
# Mais informa√ß√µes
- [Engenharia de Prompts](https://medium.com/blog-do-zouza/genai-o-que-%C3%A9-engenharia-de-prompt-6d416afe1323)
- [Dicas de Prompt - OpenAI](https://platform.openai.com/docs/guides/prompt-engineering)
- [Dicas de Prompt](https://www.promptingguide.ai/pt/introduction/basics)
- [Mais dicas de Prompts](https://medium.com/@petrusje/domine-a-arte-de-criar-prompts-eficientes-para-chatgpt-e-outras-ias-uma-base-s%C3%B3lida-b78f3b30fd9a)
