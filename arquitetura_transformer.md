# Arquitetura Transformer ü§ñ

## Desenho
<img src="https://blogdozouza.wordpress.com/wp-content/uploads/2024/11/transformes_alex.jpg" alt="Imagem WhatsApp" width="500" height="auto">

[Fonte](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)

## Vis√£o geral
Abaixo, ter√° uma vis√£o bem simples do funcionamento do transformer.

Vamos construir um exemplo com **um prompt inicial**, mostrando **todas as etapas do algoritmo**, incluindo **onde e como o modelo decide a pr√≥xima palavra**.

---

### **Prompt inicial:**  
**"O gato dorme..."**  
Esse √© o texto de entrada, e o modelo precisa prever a pr√≥xima palavra para completar a frase.

---

### **Passo a passo do Transformer com o exemplo:**

#### **1. Input Embedding**  
Cada palavra do prompt ("O", "gato", "dorme") √© convertida em um vetor num√©rico. Esses vetores representam o significado da palavra no modelo.  
**Exemplo:**  
- "O" ‚Üí `[0.5, -0.2, 0.8]`  
- "gato" ‚Üí `[1.0, 0.3, -0.4]`  
- "dorme" ‚Üí `[-0.6, 1.2, 0.7]`

---

#### **2. Positional Encoding**  
Adiciona informa√ß√µes sobre a posi√ß√£o de cada palavra no prompt, garantindo que o modelo saiba que "O" √© a primeira palavra, "gato" √© a segunda, etc.  
**Exemplo:**  
- "O" (posi√ß√£o 1): `[0.5 + 0.1, -0.2 + 0.3, 0.8 + 0.2]`  
- "gato" (posi√ß√£o 2): `[1.0 + 0.2, 0.3 + 0.6, -0.4 + 0.1]`  
- "dorme" (posi√ß√£o 3): `[-0.6 + 0.3, 1.2 + 0.7, 0.7 + 0.5]`

---

#### **3. Multi-Head Attention (Self-Attention)**  
O modelo agora calcula as rela√ß√µes entre as palavras para entender o contexto. Ele analisa como cada palavra se conecta com as outras.  
**Exemplo:**  
- "gato" tem uma rela√ß√£o forte com "dorme" (quem est√° dormindo).  
- "O" tem menos relev√¢ncia no contexto imediato.  

O modelo ajusta os vetores com base nessas rela√ß√µes.  
**Resultado:**  
- "O" ‚Üí ajustado para `[0.4, -0.1, 0.7]`  
- "gato" ‚Üí ajustado para `[1.2, 0.4, -0.5]`  
- "dorme" ‚Üí ajustado para `[-0.7, 1.3, 0.8]`

---

#### **4. Feedforward Network**  
Os vetores ajustados passam por uma rede para refinar as representa√ß√µes. Isso ajuda o modelo a capturar nuances mais complexas do texto.  
**Resultado (ap√≥s refinamento):**  
- "O" ‚Üí `[0.3, -0.2, 0.6]`  
- "gato" ‚Üí `[1.1, 0.5, -0.3]`  
- "dorme" ‚Üí `[-0.6, 1.4, 0.9]`

---

#### **5. Sa√≠da do Encoder**
Ap√≥s essas etapas, o Encoder gera representa√ß√µes contextuais das palavras da entrada, prontos para serem usados no Decodificador.

---

#### **6. Output Embedding (no Decodificador)**  
A sequ√™ncia parcial gerada at√© agora no Decodificador √© deslocada para a direita (**shifted right**), com um token especial de in√≠cio (**[START]**) sendo adicionado.  
- **Exemplo:**  
  Se a sa√≠da parcial for **"O gato dorme"**, o Decodificador recebe:  
  **[START], O, gato, dorme**.  

Cada palavra √© convertida em um vetor num√©rico:  
- "[START]" ‚Üí `[0.1, 0.2, 0.3]`  
- "O" ‚Üí `[0.5, -0.3, 0.7]`  
- "gato" ‚Üí `[1.0, 0.4, -0.2]`  
- "dorme" ‚Üí `[-0.6, 0.9, 0.8]`  

---

#### **7. Positional Encoding (no Decodificador)**  
Adiciona informa√ß√µes sobre a posi√ß√£o das palavras geradas, garantindo que o modelo saiba a ordem correta.  
**Exemplo:**  
- "[START]" (posi√ß√£o 1): `[0.1 + 0.2, 0.2 + 0.1, 0.3 + 0.3]`  
- "O" (posi√ß√£o 2): `[0.5 + 0.1, -0.3 + 0.2, 0.7 + 0.1]`  

---

#### **8. Masked Multi-Head Attention (Decodificador)**  
O Decodificador recebe as palavras j√° processadas: "O gato dorme" (Algo como: **"[START], O, gato, dorme"**)) e tenta prever a pr√≥xima palavra. Como √© "masked" (mascarado), ele s√≥ v√™ as palavras anteriores.  

**C√°lculo:**  
- Baseando-se no contexto de "O gato dorme", o modelo considera que "dorme" est√° relacionado com um lugar.  

---

#### **9. Multi-Head Attention (Cross-Attention)**  
Aqui, o Decodificador conecta o contexto do Encoder (sa√≠da do "O gato dorme") com as palavras que est√° tentando prever. Ele verifica como o contexto da entrada (quem est√° dormindo) se relaciona com poss√≠veis sa√≠das (onde est√° dormindo).  
**Resultado:**  
O modelo entende que a pr√≥xima palavra deve ser algo como **"no"**, indicando uma localiza√ß√£o.

---

#### **10. Feed Forward (no Decodificador):**
Depois da etapa de *Cross-Attention*, os vetores passam por uma rede *Feed Forward*, que ajusta e refina as representa√ß√µes antes de serem projetadas no vocabul√°rio.
Isso garante que as informa√ß√µes estejam bem processadas e preparadas para a pr√≥xima palavra ser prevista.

---

#### **11. Linear**
O vetor gerado para a palavra **"no"** cont√©m informa√ß√µes de todo o contexto aprendido pelo modelo. Isso significa que ele leva em conta:
- O que veio antes (**"O gato dorme"**).  
- O papel que **"no"** desempenha na frase.

Esse vetor pode ser algo abstrato como:  
**"no" ‚Üí `[0.2, 0.8, -0.1]`**  
Ele √© apenas uma representa√ß√£o num√©rica do que o modelo entende sobre a palavra no contexto da frase.

**O que a camada Linear faz?**

A camada Linear transforma o vetor abstrato **`[0.2, 0.8, -0.1]`** em uma pontua√ß√£o para cada palavra do vocabul√°rio. Suponha que o vocabul√°rio do modelo tenha 50.000 palavras, como:
- "sof√°"  
- "carro"  
- "mesa"  
- Etc...

A camada Linear faz o seguinte:  
- Converte o vetor **`[0.2, 0.8, -0.1]`** em outro vetor, por exemplo:  
  **`[3.0, 1.0, 0.5, ...]`**, onde cada posi√ß√£o do vetor corresponde a uma palavra no vocabul√°rio.  
  - Exemplo:
    - 1¬™ posi√ß√£o ‚Üí "sof√°" com pontua√ß√£o 3.0  
    - 2¬™ posi√ß√£o ‚Üí "carro" com pontua√ß√£o 1.0  
    - 3¬™ posi√ß√£o ‚Üí "mesa" com pontua√ß√£o 0.5  
    - ...


#### **12. Softmax (Previs√£o da Pr√≥xima Palavra)**  
As pontua√ß√µes geradas pela camada Linear n√£o s√£o probabilidades, mas valores brutos. O **Softmax** transforma essas pontua√ß√µes em probabilidades entre 0 e 1, garantindo que a soma seja 100%.  
**Exemplo:**  
- "sof√°" ‚Üí pontua√ß√£o 3.0 ‚Üí probabilidade 70%  
- "carro" ‚Üí pontua√ß√£o 1.0 ‚Üí probabilidade 20%  
- "mesa" ‚Üí pontua√ß√£o 0.5 ‚Üí probabilidade 10%  

Com essas probabilidades, o modelo escolhe a palavra com maior probabilidade, no caso, **"sof√°"**.

**Sa√≠da:**  
A palavra com maior probabilidade √© escolhida: **"sof√°"**.  

---

#### **Recome√ßo do Ciclo:**  
Agora o prompt se torna **"O gato dorme no sof√°"**, e o modelo pode continuar prevendo a pr√≥xima palavra caso seja solicitado.

---

### **Resumo do Fluxo:**
1. Entrada: **"O gato dorme"** √© processada no Encoder.  
2. Decoder prev√™ a pr√≥xima palavra: **"sof√°"**, com base no contexto.  
3. O ciclo se repete at√© completar a sa√≠da desejada.

---

### **Outros pontos:**
#### **Add & Norm:**
O **Add & Norm** √© uma etapa simples, mas essencial no Transformer. Ele tem duas fun√ß√µes principais:

1. **Residual Connection (Add):**  
   Ele **soma** a entrada original da camada com a sa√≠da processada (por exemplo, do Self-Attention ou Feedforward). Isso ajuda a preservar informa√ß√µes importantes da entrada inicial que poderiam ser perdidas no processamento.

2. **Normaliza√ß√£o (Norm):**  
   Depois da soma, ele **normaliza os valores** para manter a estabilidade do modelo e garantir que os valores n√£o fiquem muito altos ou muito baixos, facilitando o aprendizado.

**Resumo:**  
O **Add & Norm** age como um "ajuste fino" que combina o que o modelo j√° sabe (entrada) com o que ele acabou de aprender (sa√≠da), garantindo que o aprendizado continue eficiente e est√°vel.

---

Link: [Para aprofundar](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) ü§ì
